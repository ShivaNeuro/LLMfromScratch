# Building LLM from scratch

This repository contains building LLM from scratch.

## Motivation
This work is guided by video series from Vizuara.
A big shoutout to **Raj Abhijit Dandekar** and the vizuara team , **Sebastian Raschka** for his book 

This structure represents the typical flow of an LLM development process:

1. Tokenization: Breaking text into tokens.
2. Input/Target Pairs: Preparing training data.
3. Vector Embeddings: Converting tokens into numerical vectors.
4. Positional Embeddings: Adding position-aware information.
5. Attention Mechanism: Enabling context-aware processing.
6. LLM Architecture: Defining the model structure.
7. Pretraining: Training on large corpora.
8. Finetuning: Adapting the model to specific tasks.


This repo also covers finetuning of the below. 
1. Spam classification
   https://github.com/ShivaNeuro/LLMfromScratch/tree/main/8_finetuning/spam_classifier
2. Instruction based fine tuning
   https://github.com/ShivaNeuro/LLMfromScratch/tree/main/8_finetuning/instruction_finetune
