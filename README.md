# ðŸ§  Building an LLM from Scratch

Welcome to this hands-on journey through the inner workings of **Large Language Models (LLMs)**!  
This repository is a step-by-step walkthrough of the entire LLM pipeline â€” from raw text to a fine-tuned models.

---

# ðŸŽ¥ Inspired By the Best
This project is deeply inspired by the brilliant video series from **Vizuara**  
Huge shoutout to **Raj Abhijit Dandekar** and the entire Vizuara team for making complex concepts so accessible.  
Also, a big thanks to **Sebastian Raschka** â€” his book has been an invaluable guide throughout this process.

Each folder represents a key stage in the LLM development pipeline:

- **1_tokenizer**: Breaks raw text into manageable tokens.  
- **2_input_target_pairs**: Prepares the data for training by pairing inputs with expected outputs.  
- **3_vectorembeddings**: Transforms tokens into vector representations.  
- **4_positionalembeddings**: Adds positional context to embeddings.  
- **5_attentionmechanism**: Implements the magic behind context-aware understanding.  
- **6_llm_architecture**: Builds the core model structure.  
- **7_pretraining**: Trains the model on large-scale data.  
- **8_finetuning**: Refines the model for specific tasks.  

---

## ðŸ”§ Finetuning Projects
This repo also includes practical finetuning examples:

- ðŸ“¬ **Spam Classification**
  
      https://github.com/ShivaNeuro/LLMfromScratch/tree/main/8_finetuning/spam_classifier
  
- ðŸ“˜ **Instruction-Based Finetuning**

      https://github.com/ShivaNeuro/LLMfromScratch/tree/main/8_finetuning/instruction_finetune


