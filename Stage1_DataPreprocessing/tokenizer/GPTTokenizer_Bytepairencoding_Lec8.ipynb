{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "BytePair Encoding - Used to train LLMs such as GPT-2, GPT-3 and original model.\n",
        "Since implementing BPE can be relatively complicated , we will use an existing open source library."
      ],
      "metadata": {
        "id": "0fMLmH9s3KTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ww5QZIG4WQc",
        "outputId": "99dd1a7f-59c9-42de-f271-dc5ad88d1d49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\",importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyExuCfqNFsA",
        "outputId": "589f88e9-647c-4e6b-ad5d-e24f91a887b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows\n"
      ],
      "metadata": {
        "id": "KY0kwu7JNQSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The usage is similar to tokenizer used in lecture 7 , simpletokenizzer version2"
      ],
      "metadata": {
        "id": "P5Y-gkodNeZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> in the sunlit terraces\"\n",
        "    \"of someunknownPlace.\"\n",
        ")\n",
        "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)\n",
        "\n",
        "#someunknownPlace also is encoded as we use bytepair encoding."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfxqQ28dNUqw",
        "outputId": "08252cda-65d0-449e-aec7-44feae44373c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 287, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can convert tokenid's back to text using decode method.\n",
        "\n"
      ],
      "metadata": {
        "id": "07LwZvPJOlDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvma0qkuOsNT",
        "outputId": "68535abb-abe3-48a5-8ba9-5427fabcccd6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> in the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two noteworthy observations based on the token IDs and decoded text.\n",
        "\n",
        "First, the <|endoftext|> token is assigned a relatively large token ID, namely 50256.\n",
        "Infact, the BPE tokenizer , which was used to train models such as GPT-2, GPT-3 a, and the original model with <|endoftext|> being assigned the largest token ID."
      ],
      "metadata": {
        "id": "y2387xHFPAgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, the BPE tokenizer above encodes and decodes unknown words , such as \"someunknownPlace\".The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens?\n"
      ],
      "metadata": {
        "id": "WqMdJa45PU2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters.This enables it to handle out of vocabulary words.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5o1tVN0WPrE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Another example of how BPE deals with unknown words.\n",
        "\n",
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p9Tu_B9QH8N",
        "outputId": "e89b08d4-4fc7-4f37-bbb4-62785b83e8d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ]
    }
  ]
}