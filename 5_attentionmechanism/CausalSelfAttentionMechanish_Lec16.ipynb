{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS"
      ],
      "metadata": {
        "id": "JsuTA0RTpWmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsZsqNZ4p630",
        "outputId": "4cee5831-ffe8-42cc-b284-3f369d55f5fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "XCGpC8lJSYtJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "inputs = torch.tensor([\n",
        "    [0.43,0.15,0.89], #Your\n",
        "    [0.55,0.87,0.66], #Journey\n",
        "    [0.57,0.85,0.64], #starts\n",
        "    [0.22,0.58,0.33], #with\n",
        "    [0.77,0.25,0.10], #one\n",
        "    [0.05,0.80,0.55]]) #step"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "d_out = 2\n",
        "torch.manual_seed(123)\n",
        "\n",
        "#Weight matrices\n",
        "w_query = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "w_key = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "w_value = torch.nn.Parameter(torch.rand(d_in,d_out),requires_grad=False)\n",
        "\n",
        "print(w_query)\n",
        "print(w_key)\n",
        "print(w_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83RDviC8qTFH",
        "outputId": "f0dc16dd-a13d-487b-df44-7aae5e0bd564"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n",
            "Parameter containing:\n",
            "tensor([[0.1366, 0.1025],\n",
            "        [0.1841, 0.7264],\n",
            "        [0.3153, 0.6871]])\n",
            "Parameter containing:\n",
            "tensor([[0.0756, 0.1966],\n",
            "        [0.3164, 0.4017],\n",
            "        [0.1186, 0.8274]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we are setting requires_grad=False to reduce clutter in the outputs for illustration purposes.\n",
        "If we were to use the weight matrices for model training, we would set require_grad = True to update these matrices during training."
      ],
      "metadata": {
        "id": "MhNwBEAxrQpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next , we compute the query, key and value vectors as shown earlier."
      ],
      "metadata": {
        "id": "SjJDZ0pqrfN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_2 = x_2 @ w_query\n",
        "key_2 = x_2 @ w_key\n",
        "value_2 = x_2 @ w_value\n",
        "\n",
        "print(query_2)\n",
        "print(key_2)\n",
        "print(value_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht2zCxwNreMV",
        "outputId": "cfdbb8fc-232e-4500-b658-2616c6c7f85f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n",
            "tensor([0.4433, 1.1419])\n",
            "tensor([0.3951, 1.0037])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can obtain all keys and values via matrix multiplication\n",
        "\n"
      ],
      "metadata": {
        "id": "wrlw7t2KtTBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ w_key\n",
        "values = inputs @ w_value\n",
        "queries = inputs @ w_query\n",
        "\n",
        "print(\"keys.shape\",keys.shape)\n",
        "print(\"values.shape\",values.shape)\n",
        "print(\"queries.shape\",queries.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qaw3SKGthuz",
        "outputId": "92378b18-5e18-4a6d-e400-c332a1d1e4b0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape torch.Size([6, 2])\n",
            "values.shape torch.Size([6, 2])\n",
            "queries.shape torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urQdRhixz5Sr",
        "outputId": "14ea76e4-996e-4c25-c803-b89adc01024d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can generalize this computation to all attention scores via matrix multiplicaiton"
      ],
      "metadata": {
        "id": "5JDfvk562rPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T #All attention scores for given query\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j47GnVs2wiz",
        "outputId": "8187cc28-4223-4623-dc9a-9817572c7700"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = queries @ keys.T\n",
        "print(attn_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMlCK9Ql282X",
        "outputId": "84b49cf0-46ff-4a11-ba1e-4c5323ff7cb7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
            "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
            "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
            "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
            "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
            "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaling by square root of keys dimension before applying softmax."
      ],
      "metadata": {
        "id": "j5BNkd455-PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5,dim=-1)\n",
        "print(attn_weights_2)\n",
        "print(d_k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4jlFcrG6DJ1",
        "outputId": "450ba67c-637b-4b23-a3d0-5b123c457d58"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WHY DIVIDE BY SQRT (DIMENSION)"
      ],
      "metadata": {
        "id": "pp7FqKLh7PWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Reason 1: For stability in learning\n",
        "\n",
        "The softmax function is sensitive to the magnitudes of its inputs. When the inputs are large, the differences between the exponential values of each input become much more pronounced. This causes the softmax output to become \"peaky,\" where the highest value receives almost all the probability mass, and the rest receive very little.\n",
        "\n",
        "In attention mechanisms, particularly in transformers, if the dot products between query and key vectors become too large (like multiplying by 8 in this example), the attention scores can become very large. This results in a very sharp softmax distribution, making the model overly confident in one particular \"key.\" Such sharp distributions can make learning unstable,"
      ],
      "metadata": {
        "id": "Q5sa-dA_7Sb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the tensor\n",
        "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
        "\n",
        "# Apply softmax without scaling\n",
        "softmax_result = torch.softmax(tensor, dim=-1)\n",
        "print(\"Softmax without scaling:\", softmax_result)\n",
        "\n",
        "# Multiply the tensor by 8 and then apply softmax\n",
        "scaled_tensor = tensor * 8\n",
        "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
        "print(\"Softmax after scaling (tensor * 8):\", softmax_scaled_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owzDizjM7Xne",
        "outputId": "4a7019cb-66d3-4077-95ed-bc7c96bf5988"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax without scaling: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
            "Softmax after scaling (tensor * 8): tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WHY SQRT ?\n",
        "Reason 2: To make the variance of the dot product stable\n",
        "\n",
        "The dot product of  Q and K increases the variance because multiplying two random numbers increases the variance.\n",
        "\n",
        "The increase in variance grows with the dimension.\n",
        "\n",
        "Dividing by sqrt (dimension) keeps the variance close to 1"
      ],
      "metadata": {
        "id": "wWjypbn87kzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Function to compute variance before and after scaling\n",
        "def compute_variance(dim, num_trials=1000):\n",
        "    dot_products = []\n",
        "    scaled_dot_products = []\n",
        "\n",
        "    # Generate multiple random vectors and compute dot products\n",
        "    for _ in range(num_trials):\n",
        "        q = np.random.randn(dim)\n",
        "        k = np.random.randn(dim)\n",
        "\n",
        "        # Compute dot product\n",
        "        dot_product = np.dot(q, k)\n",
        "        dot_products.append(dot_product)\n",
        "\n",
        "        # Scale the dot product by sqrt(dim)\n",
        "        scaled_dot_product = dot_product / np.sqrt(dim)\n",
        "        scaled_dot_products.append(scaled_dot_product)\n",
        "\n",
        "    # Calculate variance of the dot products\n",
        "    variance_before_scaling = np.var(dot_products)\n",
        "    variance_after_scaling = np.var(scaled_dot_products)\n",
        "\n",
        "    return variance_before_scaling, variance_after_scaling\n",
        "\n",
        "# For dimension 5\n",
        "variance_before_5, variance_after_5 = compute_variance(5)\n",
        "print(f\"Variance before scaling (dim=5): {variance_before_5}\")\n",
        "print(f\"Variance after scaling (dim=5): {variance_after_5}\")\n",
        "\n",
        "# For dimension 20\n",
        "variance_before_100, variance_after_100 = compute_variance(100)\n",
        "print(f\"Variance before scaling (dim=100): {variance_before_100}\")\n",
        "print(f\"Variance after scaling (dim=100): {variance_after_100}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jby9K627RzV",
        "outputId": "275a9293-fdf3-4f44-df4a-df10d6324472"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance before scaling (dim=5): 4.683106457292781\n",
            "Variance after scaling (dim=5): 0.9366212914585562\n",
            "Variance before scaling (dim=100): 107.05794087806454\n",
            "Variance after scaling (dim=100): 1.0705794087806455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvKk8oUOwxrt",
        "outputId": "b6cf36d2-a1ee-4d7a-e882-85930277b063"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTING A COMPACT SELF ATTENTION PYTHONC LASS"
      ],
      "metadata": {
        "id": "Z4REV3H-xGxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self,d_in,d_out):\n",
        "    super().__init__()\n",
        "    self.w_query = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.w_key = nn.Parameter(torch.rand(d_in,d_out))\n",
        "    self.w_value = nn.Parameter(torch.rand(d_in,d_out))\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys = x @ self.w_key\n",
        "    values = x @ self.w_value\n",
        "    queries = x @ self.w_query\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "self_attn = SelfAttention_v1(d_in,d_out)\n",
        "print(self_attn(inputs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlKeJXqUw-u7",
        "outputId": "cb88366d-8832-4aa8-c2f6-2d88540d3678"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using nn.Linear instead of random vector for key ,, query value as nn.Linear has optimized weight initialization scheme, contributing to more stable and effective model training."
      ],
      "metadata": {
        "id": "17ODfbNL0AWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias = False):\n",
        "    super().__init__()\n",
        "    self.w_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.w_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.w_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        "  def forward(self,x):\n",
        "    keys = self.w_key(x)\n",
        "    queries = self.w_query(x)\n",
        "    values = self.w_value(x)\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        "\n",
        "torch.manual_seed(789)\n",
        "self_attn_v2 = SelfAttention_v2(d_in,d_out)\n",
        "print(self_attn_v2(inputs))\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1sIGa-w0Qcs",
        "outputId": "9b0a3c3f-0b74-4f02-9681-49aebb10a9db"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HIDING FUTURE WORDS WITH CAUSAL ATTENTION"
      ],
      "metadata": {
        "id": "PFt2lwgTM-w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor([\n",
        "    [0.43,0.15,0.89], #Your\n",
        "    [0.55,0.87,0.66], #Journey\n",
        "    [0.57,0.85,0.64], #starts\n",
        "    [0.22,0.58,0.33], #with\n",
        "    [0.77,0.25,0.10], #one\n",
        "    [0.05,0.80,0.55]]) #step\n",
        "\n",
        "queries = self_attn_v2.w_query(inputs)\n",
        "keys = self_attn_v2.w_key(inputs)\n",
        "values = self_attn_v2.w_value(inputs)\n",
        "\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHFiVQfWNBNw",
        "outputId": "12164103-727b-4821-f7e8-8b9ad6dfa3ff"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using pytorch tril function to create a mask where the values above the diagonal are zero"
      ],
      "metadata": {
        "id": "L6uYV6yp_M4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length,context_length))\n",
        "print(mask_simple)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvyEnKJo_Zmq",
        "outputId": "fc5b47c0-f6af-4c08-ef75-93903e746b3b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = mask_simple * attn_weights\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcM2xYNSABEn",
        "outputId": "9a527560-c8cb-4895-b4e4-b625205e0d92"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=1,keepdim=True)\n",
        "masked_simple_norm = masked_simple/row_sums\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm8K8Ws8AuGr",
        "outputId": "fa9ede1d-b804-4d2c-d1ae-3db64d87446a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to softmax applying above, the above approach future tokens have already influenced all the inputs . This leads to data leakage problem. To avoid this , there is smarter way to do renormalization."
      ],
      "metadata": {
        "id": "L6hVfZR-BHpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(attn_scores)\n",
        "torch.triu(torch.ones(context_length, context_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3h5l_tygCDUH",
        "outputId": "d0c0ddbd-c2c9-4169-a22d-a20936d2b8da"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
            "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
            "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
            "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
            "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
            "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1.],\n",
              "        [0., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9v1cFOHCMna",
        "outputId": "db62831d-7c36-4c82-b3ce-2f8b7232aa57"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNm5Nb1IElra",
        "outputId": "ee7b7ce7-653c-4589-ccd0-0c2719ec137a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTvHeS23JMx3",
        "outputId": "69aa9ad4-615d-4d1b-82b4-438d6c101692"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DROPOUT 0.5 Example"
      ],
      "metadata": {
        "id": "iIpfd2KcNVtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "example = torch.ones(6,6)\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC7vaN6NNXon",
        "outputId": "bfd84264-e06b-41c1-ac59-d93da9838e44"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 0., 2., 2., 0.],\n",
            "        [0., 0., 0., 2., 0., 2.],\n",
            "        [2., 2., 2., 2., 0., 2.],\n",
            "        [0., 2., 2., 0., 0., 2.],\n",
            "        [0., 2., 0., 2., 0., 2.],\n",
            "        [0., 2., 2., 2., 2., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jACJkKUDN957",
        "outputId": "4e789827-ba78-4e75-d8a9-bd4009422d7d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
      ],
      "metadata": {
        "id": "EZ2h4XSxO74v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs),dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4SJ7d3-O_s9",
        "outputId": "045bdf37-214b-4c6c-fd97-b8735d39be66"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.w_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.w_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.w_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.w_key(x)\n",
        "    queries = self.w_query(x)\n",
        "    values = self.w_value(x)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1,2) # Batches will be processed sequentially\n",
        "    attn_scores.masked_fill(self.mask.bool()[:num_tokens,:num_tokens],-torch.inf) #[:num tokens to handle if token length is less than context size]\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "s9iq2y7PPw6B"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in,d_out,context_length,0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\",context_vecs.shape)\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjW_PkkMUNoX",
        "outputId": "d48595e4-d8c2-4012-f4d2-3558d90791d7"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 2])\n",
            "tensor([[[-0.5337, -0.1051],\n",
            "         [-0.5323, -0.1080],\n",
            "         [-0.5323, -0.1079],\n",
            "         [-0.5297, -0.1076],\n",
            "         [-0.5311, -0.1066],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.5337, -0.1051],\n",
            "         [-0.5323, -0.1080],\n",
            "         [-0.5323, -0.1079],\n",
            "         [-0.5297, -0.1076],\n",
            "         [-0.5311, -0.1066],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}