{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We now consider more realistic and useful embedding sizes and encode the input tokens into a 256-dimensional vector representation. This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,228 dimensions ) but still reeasonable for experimentation.\n",
        "Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257."
      ],
      "metadata": {
        "id": "ORrkyRtLBYtY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8jnuC9h2BSpL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tiktoken\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer =torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    for i in range(0,len(token_ids)-max_length,stride):\n",
        "      input_chunk = token_ids[i:i+max_length]\n",
        "      target_chunk = token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx],self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "LLK8QIIRQk6L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt,batch_size=4, max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  # Initialize the tokenizer\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "  #Create Dataset\n",
        "  dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "\n",
        "  #Create dataloader\n",
        "  dataloader = DataLoader(dataset, batch_size = batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "L3cYWoEWDBSN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the token_embedding_layer , if we sample data from the data loader , we embed each token in each batch into a 256-dimension batch size of 8 with four tokens each, the result will be an 8 * 4 * 256 tensor"
      ],
      "metadata": {
        "id": "JX2i3pAtDOLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "jT27RxSvEIVL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8,max_length=max_length,stride=max_length,shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs,targets = next(data_iter)\n",
        "\n",
        "print(\"Token ID's \",inputs)\n",
        "print('Inputs shape',inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq6Fvv5CDbmM",
        "outputId": "bdbf8599-f4d5-427d-a0ca-7ab2952b7fd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token ID's  tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "Inputs shape torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each token id above , we want to convert into 256 dimensional vector representation."
      ],
      "metadata": {
        "id": "VQ8RwaGKFRcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YGCgvvpG4t6",
        "outputId": "3a57866e-aaad-4654-d9d2-4ee6373a5c8b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.7699, -0.2217, -0.7221,  ...,  0.4837, -0.0489, -0.3779],\n",
            "         [ 0.1575, -0.9061,  1.0397,  ...,  0.9851,  0.4620, -0.2604],\n",
            "         [-1.6930,  0.5237, -0.5384,  ...,  1.4252, -0.1468,  1.0591],\n",
            "         [-3.0873, -0.7495,  0.0201,  ...,  1.6941, -0.2330, -0.1966]],\n",
            "\n",
            "        [[ 0.2435, -0.6470,  0.3280,  ..., -0.5284,  0.1791,  0.1485],\n",
            "         [-1.9989, -1.2344, -0.0602,  ..., -0.8507, -0.3927, -2.0589],\n",
            "         [-0.7564, -1.5067, -1.8109,  ...,  1.6640,  0.2093,  1.0949],\n",
            "         [ 0.1539, -0.2916, -0.5161,  ..., -0.3100,  0.4168,  0.7999]],\n",
            "\n",
            "        [[-1.6543,  0.1814,  0.8535,  ...,  1.5126,  1.2118,  1.2212],\n",
            "         [ 2.4627,  0.8606,  0.3645,  ...,  0.4616, -0.3652,  0.5303],\n",
            "         [-0.6462,  1.3239,  0.0125,  ...,  0.3837, -0.1584,  0.9718],\n",
            "         [-0.2964, -0.7377, -0.3283,  ..., -1.0249,  0.4976,  0.5825]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.2764,  0.6392,  0.0874,  ...,  0.2428,  0.2845,  2.1805],\n",
            "         [ 1.5009,  1.1102, -1.0500,  ...,  1.5171, -0.6490,  0.2287],\n",
            "         [-1.0972, -0.3769,  0.4984,  ...,  0.6356, -1.2674, -1.4773],\n",
            "         [-1.4072, -1.4780,  1.4611,  ..., -0.5198,  0.8145,  1.2297]],\n",
            "\n",
            "        [[ 0.8771,  0.0758, -0.7459,  ...,  0.5904,  0.1834, -2.9369],\n",
            "         [-1.1710, -0.9072, -0.8847,  ..., -1.1097,  0.2091,  0.4813],\n",
            "         [ 1.4035, -0.5736, -0.1816,  ..., -1.0889,  0.7252, -1.2851],\n",
            "         [ 2.3401, -0.3490, -1.8209,  ...,  0.4255, -0.0960, -1.0542]],\n",
            "\n",
            "        [[ 1.4035, -0.5736, -0.1816,  ..., -1.0889,  0.7252, -1.2851],\n",
            "         [-0.4658,  0.9347, -0.0750,  ...,  0.2598,  0.3093, -1.4279],\n",
            "         [ 0.9742, -1.8903, -1.2545,  ...,  2.1739,  0.4532, -0.5945],\n",
            "         [ 1.5662, -0.9970,  1.3182,  ..., -1.2258, -0.4544, -2.0477]]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding for positional encoding."
      ],
      "metadata": {
        "id": "ZhJZN8ikH72Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length)) #create sequence of 4 numbers\n",
        "print(pos_embeddings)\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcA4bRTnIAH2",
        "outputId": "e9a91487-467c-4b44-8674-59bd2e722294"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3911, -0.1900, -0.0431,  ...,  0.4699,  3.0497,  0.9878],\n",
            "        [-0.0870,  0.4230,  0.0718,  ...,  0.7828,  0.4820, -0.5926],\n",
            "        [-0.3907,  0.7725,  0.2031,  ...,  1.0584,  0.5014,  0.0791],\n",
            "        [ 1.4385, -0.2545, -0.4674,  ...,  0.8664, -0.2904, -1.6673]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings)\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWaUb_9qZDOA",
        "outputId": "f84007cf-eaa5-4fa3-a4df-a595d4b2123f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.1610e+00, -4.1173e-01, -7.6517e-01,  ...,  9.5356e-01,\n",
            "           3.0008e+00,  6.0993e-01],\n",
            "         [ 7.0466e-02, -4.8314e-01,  1.1114e+00,  ...,  1.7679e+00,\n",
            "           9.4392e-01, -8.5293e-01],\n",
            "         [-2.0837e+00,  1.2963e+00, -3.3534e-01,  ...,  2.4836e+00,\n",
            "           3.5463e-01,  1.1382e+00],\n",
            "         [-1.6488e+00, -1.0040e+00, -4.4728e-01,  ...,  2.5605e+00,\n",
            "          -5.2331e-01, -1.8640e+00]],\n",
            "\n",
            "        [[ 6.3464e-01, -8.3701e-01,  2.8495e-01,  ..., -5.8502e-02,\n",
            "           3.2288e+00,  1.1362e+00],\n",
            "         [-2.0859e+00, -8.1140e-01,  1.1564e-02,  ..., -6.7828e-02,\n",
            "           8.9275e-02, -2.6515e+00],\n",
            "         [-1.1472e+00, -7.3419e-01, -1.6078e+00,  ...,  2.7224e+00,\n",
            "           7.1074e-01,  1.1740e+00],\n",
            "         [ 1.5924e+00, -5.4611e-01, -9.8343e-01,  ...,  5.5640e-01,\n",
            "           1.2641e-01, -8.6744e-01]],\n",
            "\n",
            "        [[-1.2632e+00, -8.6432e-03,  8.1044e-01,  ...,  1.9825e+00,\n",
            "           4.2614e+00,  2.2090e+00],\n",
            "         [ 2.3756e+00,  1.2836e+00,  4.3629e-01,  ...,  1.2444e+00,\n",
            "           1.1676e-01, -6.2217e-02],\n",
            "         [-1.0369e+00,  2.0965e+00,  2.1564e-01,  ...,  1.4421e+00,\n",
            "           3.4299e-01,  1.0509e+00],\n",
            "         [ 1.1420e+00, -9.9222e-01, -7.9572e-01,  ..., -1.5847e-01,\n",
            "           2.0722e-01, -1.0848e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 6.6754e-01,  4.4921e-01,  4.4353e-02,  ...,  7.1270e-01,\n",
            "           3.3341e+00,  3.1683e+00],\n",
            "         [ 1.4139e+00,  1.5332e+00, -9.7821e-01,  ...,  2.3000e+00,\n",
            "          -1.6700e-01, -3.6388e-01],\n",
            "         [-1.4879e+00,  3.9568e-01,  7.0148e-01,  ...,  1.6939e+00,\n",
            "          -7.6606e-01, -1.3982e+00],\n",
            "         [ 3.1294e-02, -1.7325e+00,  9.9371e-01,  ...,  3.4655e-01,\n",
            "           5.2416e-01, -4.3761e-01]],\n",
            "\n",
            "        [[ 1.2683e+00, -1.1424e-01, -7.8896e-01,  ...,  1.0603e+00,\n",
            "           3.2331e+00, -1.9491e+00],\n",
            "         [-1.2580e+00, -4.8424e-01, -8.1292e-01,  ..., -3.2688e-01,\n",
            "           6.9106e-01, -1.1124e-01],\n",
            "         [ 1.0128e+00,  1.9891e-01,  2.1512e-02,  ..., -3.0526e-02,\n",
            "           1.2266e+00, -1.2059e+00],\n",
            "         [ 3.7786e+00, -6.0355e-01, -2.2883e+00,  ...,  1.2919e+00,\n",
            "          -3.8632e-01, -2.7215e+00]],\n",
            "\n",
            "        [[ 1.7946e+00, -7.6363e-01, -2.2466e-01,  ..., -6.1905e-01,\n",
            "           3.7749e+00, -2.9728e-01],\n",
            "         [-5.5277e-01,  1.3577e+00, -3.2568e-03,  ...,  1.0426e+00,\n",
            "           7.9130e-01, -2.0205e+00],\n",
            "         [ 5.8344e-01, -1.1178e+00, -1.0514e+00,  ...,  3.2323e+00,\n",
            "           9.5456e-01, -5.1539e-01],\n",
            "         [ 3.0046e+00, -1.2515e+00,  8.5085e-01,  ..., -3.5942e-01,\n",
            "          -7.4476e-01, -3.7150e+00]]], grad_fn=<AddBackward0>)\n",
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    }
  ]
}